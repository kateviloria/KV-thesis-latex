\documentclass[11pt, a4paper]{article}

\usepackage{mlt-thesis-2015}

% With Xetex/Luatex this shouldn't be used
%\usepackage[utf8]{inputenc}

\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{setspace}
\usepackage{subfiles} 
\usepackage{hyperref}
\usepackage{booktabs}
%\graphicspath{ {figures/} }

\hypersetup{
colorlinks,
allcolors={blue}
}


\title{Wanna Be On Top?}
\subtitle{The Hyperparameter Search for Semantic Change's Next Top Model}
\author{Kate Viloria}

\begin{document}

%% ============================================================================
%% Title page
%% ============================================================================
\begin{titlepage}

\maketitle

\vfill

\begingroup
\renewcommand*{\arraystretch}{1.2}
\begin{tabular}{l@{\hskip 20mm}l}
\hline
Master's Thesis: & 30 credits \\
Programme: & Master’s Programme in Language Technology\\
Level: & Advanced level \\
Semester and year: & Spring, 2021\\
Supervisor: & Simon Hengchen\\
Examiner: & Eleni Gregoromichelaki\\
Keywords: & semantic change, language change, diachronic word embeddings
\end{tabular}
\endgroup

\thispagestyle{empty}
\end{titlepage}

%% ============================================================================
%% Abstract
%% ============================================================================
\newpage
\singlespacing
\section*{Abstract}


Lexical semantic change (LSC) detection through the use of diachronic corpora and computational methods continues to be a prevalent research area in language change \citep{tahmasebi-survey2018}. However, there has not yet been (to the best of our knowledge) extensive work further examining the models being trained and creating a foundation for what hyperparameter settings yield the best results. In this thesis, a large-scale hyperparameter search is conducted using the SemEval-2020 Task 1 dataset that includes English, German, Swedish, and Latin. Alongside model hyperparameters, different algorithms (Word2Vec and FastText) and alignment methods (Orthogonal Procrustes and Incremental Training) were also included. The hyperparameters evaluated are: number of training epochs, vector dimension, frequency threshold, and shared vocabulary size for the Orthogonal Procrustes alignment method. By amalgamating all of the results and assessing how model performance is affected if one hyperparameter is changed, considerations that must be made before training a model were substantiated. This research concludes that improvements in performance significantly decreases after 50 epochs during training and that the typical choice of 300 dimensions for vectors (based on English best practices in NLP) does not necessarily apply to other languages. It is also shown that choices in vector dimension, frequency threshold, and shared vocabulary size depend on the language in question, corpus size, and text genre composition. 


%\cmtSH{citep{tahmasebi-survey2018}} 
%\cmtSH{to the best of our knowledge}
%\cmtSH{performance improvement rate?}

\thispagestyle{empty}

%% ============================================================================
%% Preface
%% ============================================================================
\newpage
\section*{Preface}

I, first and foremost, would like to thank my supervisor Simon Hengchen for his unwavering support and patience throughout this process. Thank you for answering the extensive amounts of questions and concerns I have week after week. Your knowledge, experience, and guidance have been integral to my work. I am so grateful for everything that I have learned about semantic change and the ins and outs of being a researcher, but most importantly, the importance of taking breaks. 

Thank you to Merle Pfau, Konstantinos Mavromatakis, Dr. Barbara McGillivray, and Saga Hansson for taking the time to annotate the target word lists for German, Latin, and Swedish. A large portion of my analysis would not have been possible without your contributions. 

I would also like to thank Robert Adesam for his assistance in my endeavour to train 4000 models on the MLTGPU, Adam Ek for his wisdom on FastText best practices, and Nikolai Ilinykh for his constant guidance throughout the second year of my masters’ programme. 

To my fellow MLT students, thank you for making these past two years so wonderful. I am so excited to see how each and every one of you will make the world a better place. 

And of course, thank you to my friends and family for lending me their ears. A part of my personality will now be lost as this thesis comes to an end.


\thispagestyle{empty}

%% ============================================================================
%% Contents
%% ============================================================================
\newpage

\begingroup
\hypersetup{linkcolor=black} % This ensures the ToC has black links
\begin{spacing}{0.0}
\tableofcontents
\end{spacing}
\endgroup

\thispagestyle{empty}

%% ============================================================================
%% Introduction
%% ============================================================================
\newpage
\setcounter{page}{1}

\include{sections/1-introduction}

%% TODO:
% write motivation
% proofread subsection 3
% etc

%% ALREADY DONE:
% write xyz
% fixed bibtex
% etc

%% You can add and edit these comments as you see fit for the other sections, or use some other tool

\newpage

\include{sections/2-background}

\newpage

\include{sections/3-exp-setup}

\newpage

\include{sections/4-results}

\newpage

\include{sections/5-ethics}

\newpage

\include{sections/6-limitations}

\newpage

\include{sections/7-future}

\newpage

\include{sections/8-conclusion}





\addcontentsline{toc}{section}{References}
\bibliography{anthology,katesbib}

\newpage
\appendix
\input{sections/9.1-appendix-resources}

\newpage

\input{sections/9.2-appendix-overallresults}

\newpage

\input{sections/9.3-appendix-postagresults}
\newpage

\end{document}


