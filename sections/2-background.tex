\section{Background}
% subsections are \subsection{title}
%% subssubsections are \subsubsection{title}
%% numbering will work automatically

\subsection{2.1 - Distributional Hypothesis}

The Distributional Hypothesis theoretically drives the current and leading models for detecting LSC. The rationale being that “there is a correlation between distributional similarity and meaning similarity, which allows us to utilize the former in order to estimate the latter”—in simpler and more familiar terms, “words which are similar in meaning occur in similar contexts.” \citep{sahlgren2008distributional} The distributional methodology presented in \citet{harris1970distributional} is built on structuralist theory. A structuralist approach to language focuses on the general construction of a language system rather than the idiolectal use of language. According to \citet{sahlgren2008distributional}, Saussure identifies the functional differences of linguistic meaning into syntagmatic and paradigmatic relations. Syntagmatic relations involve the syntactic positioning or sequence of words. The combination and order of linguistic entities form a syntagmatic relationship that then creates meaning. Paradigmatic relations are between words that appear in the same context but do not co-occur. Given these characteristics, linguistic entities that have a paradigmatic relationship should be interchangeable within the same context or sentence. \citet{sahlgren2008distributional} offers the refined distributional hypothesis as “A distributional model accumulated from co-occurrence information contains syntagmatic relations between words, while a distributional model accumulated from information about shared neighbors contains paradigmatic relations between words.” With this in mind, models with a larger context window are more likely to detect or learn paradigmatic relations. Depending on the manipulation of specific model hyperparameters, certain relations will be learned. 


\subsection{2.1 - Lexical Semantic Change(??)}
LSC detection through computational methods still face many challenges today. \citet{hengchen2021challenges} have identified two approaches in the computational field of LSC—treating a word as an entity and determining semantic change based on its dominant sense and treating each word’s sense as a separate entity. Both approaches however, mostly capture contextual similarity between lexical items while the different levels of meaning are seldom distinguished \citep{hengchen2021challenges}. There are three different methods to model the meaning of a word computationally: each word in the vocabulary and all its semantic information will have one representation (e.g., static embeddings), each word being split into different semantic areas resulting in representations that approximate a word’s senses (e.g., topic modelling), and each word having a representation for every time it is used in a sentence (e.g., contextual embedding). Each method can be successful given it is applied appropriately in different tasks and what kind of LSC problem is trying to be solved. It is also important to note that not all approaches are able to model or differentiate the different senses a word could possess. In addition to the word sense limitations these representations have, count representations have also been proven to introduce an inherent dependence on word frequency—resulting in random noise in the models \citep{dubossarsky-etal-2017-outta}. By comparing different corpora with each other, original and shuffled, \citet{dubossarsky-etal-2017-outta} demonstrates that there is a strong correlation between the change scores of words and their frequencies. 

Once words have been created into representations, there are also different calculations that can be used when comparing word representations between two time periods (e.g. cosine distance, Euclidean distance, Jensen-Shannon divergence, etc.). With these calculations, there are two ways to evaluate large datasets and tasks involving the detection of LSC—binary LSC (i.e. has a word changed or not) or a graded LSC (i.e. to what degree has a word changed). Although these approaches present a systematic way of evaluating the current models of meaning being created, the question of what kind of change in meaning has a word undergone remains unanswered.

Corpora and datasets used in the field of LSC are largely in English. However, to assume that semantic change occurs in similar ways across other languages is extremely incorrect. As stated by \citet{bender_2020}, the advancements in NLP (ACRONYM) rely on the existence of language resources and English is neither synonymous with nor representative of “Natural Language”. English, as a high resource language, naturally results in more research being published on English. The field of LSC is no different. Apart from a handful of corpora in different languages (\citet{diacrita_evalita2020} for Italian and \citet{rushifteval2021} for Russian), most research in LSC are conducted on the English language. In order to study semantic change effectively and successfully, the variation that already exists between languages must be considered. The creation of resources for languages other than English is crucial to the development of LSC and in turn (NOT USING RIGHT?), NLP.   