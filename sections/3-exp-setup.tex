\section{Experimental Setup}

\subsection{3.1 - Dataset}

The SemEval-2020 Task 1 on Unsupervised Detection of Lexical Semantic Change \citep{schlechtweg-etal-2020-semeval} provides manually-annotated datasets for four languages (English, German, Swedish, and Latin). It is composed of a diachronic corpus pair and a set of target words for each language. Having a gold standard that is based on roughly 100,000 human judgments, researchers now have a more concrete foundation for comparing models. There are two subtasks that differ by the assessment of LSC: binary classification and ranking. In order to identify and evaluate the subtle effects of hyperparameter changes, this thesis follows Subtask 2—based on a ranking of the target words depending on the degree of LSC between the first and second corpus. In contrast to Subtask 1’s binary classification, the method of ranking in Subtask 2 “captures fine-grained changes in the two sense frequency distributions” \citep{schlechtweg-etal-2020-semeval}. The two corpora for each language (C1 and C2) were divided based on data size and the availability of target words. Pre-processing of all corpora involved lemmatization, removal of all punctuation, and randomly shuffling sentences within each time-specific corpora.

The Clean Corpus of Historical American English (CCOHA) consists of different types of text—fiction, non-fiction, magazines, and newspapers—from the 1810s to the 2000s \citep{davies2012expanding, alatrash-etal-2020-ccoha}. The first time-bin corpus used for German is Deutsches Textarchiv (DTA) which is composed of different genres of text from the 16th-20th centuries \citep{dta2017}. For the second time-bin, a combination of the Berliner Zeitung (BZ) and Neues Deutschland (ND) corpora is used \citep{berliner2018,neues2018}. Both German corpora are comprised of newspaper articles from the years of 1945-1993. The corpus used for Latin, LatinISE \citep{mcgillivray-kilgarriff}, is a compilation of texts originating from 2nd century B.C. to 21st century AD from three online digital libraries. The corpora used for Swedish is the Kubhist corpus \citep{Kubhist}, which similar to the German corpus used, is a newspaper corpus with articles from the 18th to the 20th century. \hfill \break
\begin{table}[h]
\small
\centering
\begin{tabular}{l|cc|cc|}
\cline{2-5}
\textbf{}      & \multicolumn{2}{c|}{\textbf{$C_1$}}                    & \multicolumn{2}{c|}{\textbf{$C_2$}}                    \\
                                       & \textit{\textbf{corpus}} & \textit{\textbf{period}} & \textit{\textbf{corpus}} & \textit{\textbf{period}} \\ \hline
\multicolumn{1}{|l|}{\textbf{English}} & CCOHA                    & 1810-1860                & CCOHA                    & 1960-2010                \\ \hline
\multicolumn{1}{|l|}{\textbf{German}}  & DTA                      & 1800-1899                & BZ + ND                  & 1946-1990                \\ \hline
\multicolumn{1}{|l|}{\textbf{Latin}}   & LatinISE                 & -200-0                   & LatinISE                 & 0-2000                   \\ \hline
\multicolumn{1}{|l|}{\textbf{Swedish}} & Kubhist                  & 1790-1830                & Kubhist                  & 1895-1903                \\ \hline
\end{tabular}
\caption{Time periods of each sub-corpora for each language.}
\label{tab:subcorpora-time}
\end{table}
\hfill \break
Annotators—all native speakers or former university students of the respective language they were assigned—were instructed to follow the DURel framework \citep{DURel2018}. Deriving from \citet{blank1997prinzipien}’s continuum of semantic proximity for synchronic polysemy annotation, its semantic-relatedness scale for a target word \emph{w} within two specific time periods from C1 and C2 resulted in high inter-annotator agreement. 
	
Each language is accompanied by a target word list that consists of words that have undergone semantic change or stable words—words that have not changed in meaning. For words that have changed in meaning, it is not distinguished whether words have lost or gained a sense. Stable words were chosen to act as counterparts of words that have changed in meaning through the consideration of the same POS tag and a comparable/similar frequency development between the two time periods. This consideration minimizes possible model biases that result from these factors. \citep{dubossarsky-etal-2017-outta} Since many of the English target words underwent POS-specific semantic changes, POS tags have been concatenated in the target word list (“word\_pos”). Although the addition of POS tags in the SemEval task was for the purpose of English words having a tendency to change POS tags when changing senses, it was also a great opportunity to examine model performances based on POS tags for this project. The addition of POS tags for the target word lists of the remaining three languages will be discussed further below.
 
% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[h]
\small
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Language} & \textbf{NN} & \textbf{VB} & \textbf{ADJ} \\ \hline
\textit{English}                          & 33          & 4           & 0            \\ \hline
\textit{German}                           & 32          & 14          & 2            \\ \hline
\textit{Latin}                            & 28          & 5           & 7            \\ \hline
\textit{Swedish}                          & 22          & 6           & 3            \\ \hline
\end{tabular}
\caption{Number of nouns (NN), verbs (VB), and adjectives (ADJ) in each language's target word list.}
\label{tab:postag-breakdown}
\end{table}

(FROM MOTIVATION)
\cmtKV[inline]{this section feels out of place but at the same time should be mentioned, just not sure if it's better off in motivation}
Another challenge that the field is currently facing is having a more robust system of evaluation. Currently, semantic annotation stands as the most reliable (sole (TECHNICALLY NOT)) way of evaluating and validating semantic change in historical corpora \citep{hengchen2021challenges}. Although effective (for now), the annotating process to obtain ground-truth data results in large amounts of money and time expended. It also only produces a limited target word list—allowing the possibility of evaluation inconsistencies to be introduced. A model might perform well with one target list and horribly with another. (CONJECTURE? LOL) Using a simulated LSC for evaluation is in its early stages and is encouraged to be used for evaluation in tandem with ground-truth testing.

Numerous types of semantic representations were used by teams during the SemEval task: token embeddings vs. type embeddings and topic modelling vs. vector space models. In both Subtask 1 and 2, the highest-performing systems used static-type embedding models. Token embeddings include more contextual information each time the target word appears in the corpus while type embeddings are average embeddings (LACKING). However, it was pointed out that raw corpora, rather than lemmatized corpora, would bode better for token embeddings. (ANNOTATION?) Following the best-performing models of the tasks, with a larger focus on Subtask 2, a hyperparameter search of models with type embeddings would help determine if the top scores of the task could be surpassed solely on changes in hyperparameters. (ENDSOUNDS WEIRD, EDIT) 

\subsection{3.X - Experimental Large-Scale Hyperparameter Search (TOO LONG?)}
In order to clarify the scope and justify the hyperparameter range for this large-scale hyperparameter search, the results from the SemEval-2020 Task 1 are used as the basis/starting point (PICK ONE). The top-performing models were taken into consideration along with the post-hoc improvements and analyses of those models by \citet{kaiser-etal-2020-ims}. The pipeline was coded and structured with the possibility of reusing the code for conducting similar hyperparameter searches in future LSC tasks with the hopes of the hyperparameter searches to be based on the findings of this thesis—and in turn, hyperparameters with smaller ranges and motivated by linguistic and domain-specific reasoning. 

\cmtKV[inline]{**formatting, also think it would be useful to show the range after each subsubsection heading or would that be redundant, this below could also be neater, need to look into it}
The initial hyperparameters examined and analysed are shown below:
\begin{itemize}
    \item language $\in$ [English, German, Latin, Swedish],
    \item algorithm $\in$ [Word2Vec, FastText],
    \item alignment method $\in$ [Orthogonal Procrustes, Incremental Training],
    \item epochs $\in$ $[5, 10, 20, 50, 100]$,
    \item dimensions $\in$ $[10, 25, 50, 100, 300]$,
    \item frequency threshold $\in$ $[5, 10, 50, 100]$.
\end{itemize}%\vspace{+1ex}


**READABILITY SAKE, not canonically hyperparameters but are included when mentioning combination of hyperparameter

\subsubsection{Algorithm}
Type embeddings were used in the top-performing models for all four languages in Subtask 2.  Since type embeddings overwhelmingly outperformed token embeddings, static-type embeddings were chosen to be implemented. The same is seen in the hyperparameter search conducted by \citet{hengchen2021SBXrushifteval} with Russian data. The skip-gram negative sampling (SGNS) algorithm was used by the top 4 teams whose models performed well in Subtask 2 of the SemEval task. Bidirectional Encoder Representations from Transformers (BERT) was not used as \citet{laicher-2020} demonstrated that it resulted in poor results for an LSC task in Italian. Variety is provided in the algorithm hyperparameter by comparing the use of n-grams of Word2Vec and FastText. Word2Vec has shown success in LSC tasks and the use of FastText would address the morphological considerations for words that should be made as shown by \citet{bojanowski2017enriching}.

\subsubsection{Alignment Method}
The two alignment methods used are Orthogonal Procrustes and incremental training. Orthogonal Procrustes is implemented by using the first time period model as the base and the second time period model is “stretched” to fit the first. This is motivated by the fact that language builds on itself. In order to make comparisons of words changing over time, new usages of already existing words and new words (second time period model) should be made to fit into the contexts and usages that it proceeds (first time period model). The Gensim Word2Vec Procrustes Alignment tutorial by Ryan Heuser\footnote{\url{https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf}}  was initially used but was not compatible with FastText models. These aligned models were still saved and evaluated. However, all models were then aligned following the implementation of Team UWB @ SemEval \citet{prazak-etal-2020-uwb}’s model implementations using VecMap \footnote{\url{https://github.com/artetxem/vecmap}} by \citet{artetxe2018generalizing}. However, implementation differed from \citet{prazak-etal-2020-uwb}’s since the alignment chosen was supervised due to computational requirements and preliminary tests showed good performance. After seeing the results of VecMap’s supervised alignment method, another hyperparameter was introduced for models that are being aligned through Orthogonal Procrustes. There was an option on how many words can be provided in the training dictionary where the word vectors can be aligned automatically. Given a word \emph{w} from the first time period model, its equivalent to the second time period model would also be w. The additional hyperparameter was the amount of words given to VecMap as equivalents—making aligning the two models “easier” or “more supervised” (WORDING).

Models that have Orthogonal Procrustes as the alignment method will also have this added hyperparameter:
\begin{itemize}
    \item vocabulary size $\in$ $[1000, 5000, 10000, ALL]$
\end{itemize}%\vspace{+1ex}

The second alignment method, incremental training, was implemented by taking the first time period model and continue its training but this time using the second time period corpus. The models being compared will be the model trained on the first time period corpus and the model trained on both the first and second time period corpus.

\subsubsection{Epochs} \label{exp-epochs}
Epochs chosen were initially a very wide range from 5 to 100. A smaller preliminary test for higher epochs were conducted while training Word2Vec to see if there were significant improvements in performance and if it was worth exploring further. It was decided that FastText models trained on 100 epochs should not be pursued further due to computational costs and no added performance. The litmus test was done on Word2Vec models as it will always have less vectors since it operates on a per-word basis rather than FastText which creates vectors based on character \emph{n}-grams. 

\subsubsection{Dimensions}
Variety in dimensions were included in this hyperparameter search due to \citet{kaiser-etal-2020-ims}’s post-hoc analysis of the top-performing models in the SemEval-2020 Task 1 demonstrating that the same models can be outperformed by optimising *vector initialization alignment* (INCLUDE?) dimensionality. Their results also show that frequency-induced noise is introduced through vector space alignment and it is strongly correlated to dimensionality. With higher dimensions, vector representations could learn more specific contexts and usages of the word. However, high vector dimensions may also lead to more noise being introduced in the word representations. 

\subsubsection{Frequency Threshold}
Frequency thresholds can have a significant effect on computational time as well as overall performance. The frequency threshold is the amount of times a word has to appear in a corpus for it to be included in the training of the model. This threshold can either help eliminate words that are not relevant and only introduce noise but it can also eliminate important contexts that could help with the nuanced usages of words, possibly a target word. Exploring this hyperparameter would allow us to see if this hyperparameter has an optimal number when combined with other hyperparameters. 

\subsection{3.X - Evaluation/Spearman's Rank Order Correlation}
For SemEval-2020 Subtask 2 where LSC is quantified as a measurement of change through cosine distances, Spearman’s Rank Order Correlation is calculated as the main statistical measure of overall score. Once the cosine distances are calculated for each target word, they are ranked and compared with the gold truth labels. Spearman's correlation $\rho$ is then calculated along with its p-value. Spearman’s $\rho$ can be any value from -1 to 1 where 1 is a perfect positive correlation between ranks, -1 is a perfect negative correlation between ranks, and 0 is no correlation between ranks. In this task, the best possible score for a model would be 1, meaning that it has ranked the cosine distances exactly like the truth labels. Calculating through ranking rather than direct comparisons (such as the difference between each target word’s cosine distance) was chosen since it allows vector spaces to vary more in size given the large amount of hyperparameter combinations being implemented. (NOT SURE IF THIS IS TRUE OR JUST WORDED BADLY) The same evaluation will be done for each model but with the target words divided by POS-tag. Each model will have a Spearman’s $\rho$ for nouns, verbs, and adjectives. 

\subsection{3.X - Ethical Considerations and Efficiency Adjustments}
Initially, 1600 models were planned to be trained for this large-scale hyperparameter search. However, with the deduction of 100-epoch models for FastText and the addition of the vocabulary alignment hyperparameter for the Orthogonal Procrustes alignment method, 4000 models were trained in total. Throughout the training process, it was important to vigilantly take into consideration computational times, storage, and energy usage. As stated above in EPOCHS SECTION, 100-epoch models were trained on Word2Vec first and examined to see if the increase in epochs and therefore computational time and energy used also reflected in the model’s performance. Since it did not show promising results or results that increased in conjunction with time and energy spent, training 100-epoch models for FastText was deemed not worth pursuing further. 

The longer period of planning the pipeline was to ensure that once training begins, there are as little road blocks as possible. This would ensure that the training process and the storing of models were always organised and never lost or having to be retrained to avoid unnecessary consumption of resources. In hopes of remaining vigilant and conscious of the large amount of energy, resources, and storage being used for this thesis, adjustments to help with time and energy consumption were constantly being made to the pipeline. After the large-scale pipeline and the Orthogonal Procrustes alignment was incompatible with FastText models, the pipeline was restructured and already existing models were reformatted for efficiency purposes. Before the reformatting, 1TB of data and models were being stored. They were then transferred and reformatted as text files. To save computational time and energy, the cosine distances for the target words of each model were stored as text files and will be made available to the public along with the model text files and pipeline. This was incredibly helpful since the initial plan was for the model to iterate through the entire model to search for the target words and calculate the overall Spearman’s $\rho$ and again for nouns, verbs, and adjectives. Another adjustment made during training was for the incremental alignment method. The model trained on the first time period corpus with the same hyperparameter combinations but for the Orthogonal Procrustes alignment method was used and trained again with the second time period corpus. This eliminated training what would have been 720 duplicate models.  

This large undertaking was done using an Intel Xeon CPU E5-2640 v4 (25M Cache, 2.40 GHz). With about X hours total of training and computational time, around XX kg of CO\textsubscript{2} has been used which equates to X bananas, or X kilometers driven, or X avocados from Mexico.\footnote{\url{https://www.weforum.org/agenda/2020/02/avocado-environment-cost-food-mexico/}} These calculations were made through the CO\textsubscript{2} GU mltgpu tutorial\footnote{\url{https://github.com/faustusdotbe/CO2_GU_mltgpu/blob/main/mltgpu_co2.ipynb}} by Simon Hengchen. These statistics are incredibly important to disclose considering the vast environmental impact conducting these experiments will have. 
\citet{strubell-etal-2019-energy} states that the most computationally-hungry models typically obtain the highest scores. \citet{strubell-etal-2019-energy} also raises awareness on the CO\textsubscript{2} consumption within the NLP community and the importance of reporting of train times and sensitivity to hyperparameters. By providing these statistics, the overall scores, and other files that require computational time and energy consumption, others are able to  further explore and examine this task without having to do all of the model training again. The reporting and analysing of hyperparameter sensitivities and patterns between the models will also allow researchers to apply the intuition when selecting hyperparameters in order to train models that are efficient, environmentally conscious, and yield the best results.

