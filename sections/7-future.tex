\section{Future Work}
\label{sec:futurework}

There are still many questions that remain unanswered in the study of language change. In detecting LSC, finding methodologies that perform well in their respective domains is a continuous undertaking. Ways to expand the hyperparameter search in different aspects will be discussed in this section.

Models trained on diachronic corpora to detect LSC must also be validated in other ways. Although the top models in this thesis performed well based on SemEval’s diachronic corpora, it would be interesting to take the same models and examine the model’s word representations based on synchronic data. Evaluating the validity of these models not only diachronically would give more insight into what word representations have learned and how accurate they are. This can be done by using a dataset such as \citet{supersim2021}'s SuperSim dataset—a similarity and relatedness test set for Swedish based on expert human judgments. 

An evident expansion of this hyperparameter search would be to use the top-performing hyperparameter combinations on other tasks and datasets to assess their robustness and applicability. Conducting similar hyperparameter searches for other languages would allow for a results comparison to see if the same hyperparameter combinations perform best and persist with languages that have similar structures or belong to the same language family. Of course, it is important to keep in mind that availability of datasets for LSC is very limited and the work will begin with creating these datasets. However, once this work has been done, identifying patterns between languages and their top-performing models would allow for a higher-level examination of hyperparameter combinations. Conclusions will hopefully be possible for meta-languages depending on how languages are related—genetic or phylogenetic, or shared language features (e.g. phonology, morphology, syntax). This would allow for more broad intuitions into what models create reliable word representations depending on the language.

On a smaller scale, an examination of the models that performed the worst during this hyperparameter search would also shed some light on what combinations \emph{not} to use. Analysing the worst model combinations in the same method as done with the top-performing models in Section~\ref{sec:results} might shed some light on what makes models unsuccessful. 

