\section{Ethical Considerations}
\label{sec:ethicalcons}

Alongside an experimental research method such as a hyperparameter search, ethical considerations and acknowledgments must be included. The effects of conducting this experiment is critically assessed and evaluated through the lens of environmental and sociological impact. (SOCIOLOGICAL ASSUMPTIONS MADE? NOT REALLY IMPACT) 

Throughout the process of the hyperparameter search, environmental impact was considered at each step. Although making adjustments were made in order to minimise environmental impact, they also create a more efficient and robust pipeline. As discussed in \autoref{exp-ethics} AUTOREF CO2 , the project had a total of X hours of training and computational time spent and X amount of CO\textsubscript{2} consumed. After training all of the model combinations that use the Word2Vec algorithm, results were evaluated and it was decided that not training 160 model combinations that required 100 epochs of training for FastText would not produce results as significant or meaningful. Since FastText aims to create character \emph{n}-gram vector representations, training 100-epoch models would involve a much larger computational time and memory usage compared to the already large models that were created using Word2Vec. Despite acknowledgments and modifications for minimising consumption and efficiency reasons, a significant amount of energy and memory being used is inevitable. To ensure that the energy consumed and the space taken is worthwhile, all of the results and pipeline will be available for public access (see \autoref{app-resources}). The POS-tagged word lists and cosine distances for each model will also be available for further analysis and evaluations. This large-scale hyperparameter search is also well-documented to ensure that it will not have to be done again and future tasks can focus on refining model optimisations. 

Another crucial consideration that must be made is regarding the data and the type of content being used for training models. Humans have written the texts in the corpora being used and humans are inherently biased. Anonymity is also a concern for researchers when using text as data. Most of the historical texts used for this thesis are quite old, spanning from 2nd century B.C. to 2010 with most of the texts coming from 1800-1900s (FORMAT LOOKS WRONG). The question of anonymity and privacy is not a large concern for this thesis. (FEELS LIKE MISSING A SENTENCE) Through training, machines also inevitably learn the biases from these past texts. Biases such as racism, sexism, homophobia and other forms of discrimination towards marginalised groups of people can be seen throughout historical texts. \citet{tripodi-etal-2019-tracing} show that antisemitic language can be detected through language change. It is extremely vital to recognise that these biases can appear in historical texts and to ensure readers that the author (LOL , JUST USE I?) does not endorse, nor agree with these biases. It is incredibly important to acknowledge the inherent bias that exists within historical data and what might be passed on to trained models. As \citet{hengchen-tahmasebi_2021-swedishdiachronic} state, the hope is to “shed light on these representations, as ignoring them would mean they have never existed”. 
